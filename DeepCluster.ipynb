{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepCluster.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mq8IuIxVW-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e788e7f0-9421-4593-ae75-39ea3165cb92"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import datasets, transforms\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import StandardScaler, normalize\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "import math\r\n",
        "import copy\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "!pip install faiss-gpu\r\n",
        "import faiss\r\n",
        "\r\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/32/8b29e3f99224f24716257e78724a02674761e034e6920b4278cc21d19f77/faiss_gpu-1.6.5-cp36-cp36m-manylinux2014_x86_64.whl (67.6MB)\n",
            "\u001b[K     |████████████████████████████████| 67.7MB 46kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.6.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f8e9dc3ab58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BllDgfIse7jz"
      },
      "source": [
        "The model used in this analysis is a simple cnn model with 2 convolutional layer and two fully connected layers. The model is split into features, classifier and top_layer to mimic the architecture used in the original paper. See [here](https://github.com/facebookresearch/deepcluster/tree/master/models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQTYT7tAWk8T"
      },
      "source": [
        "class SimpleCnn(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, k=10):\r\n",
        "        \r\n",
        "        super(SimpleCnn, self).__init__()\r\n",
        "        self.features = nn.Sequential(\r\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(8),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\r\n",
        "        \r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Linear(7*7*16, 64),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.top_layer = nn.Linear(64, k)\r\n",
        "        self._initialize_weights()\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        out = self.features(x)\r\n",
        "        out = out.reshape(out.size(0), -1)\r\n",
        "        out = self.classifier(out)\r\n",
        "        if self.top_layer:\r\n",
        "            out = self.top_layer(out)\r\n",
        "        return out\r\n",
        "    \r\n",
        "    def _initialize_weights(self):\r\n",
        "        for y, m in enumerate(self.modules()):\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                for i in range(m.out_channels):\r\n",
        "                    m.weight.data[i].normal_(0, math.sqrt(2. / n))\r\n",
        "                if m.bias is not None:\r\n",
        "                    m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                m.weight.data.normal_(0, 0.01)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "def train_supervised(model, device, train_loader, epoch):\r\n",
        "    model.train()\r\n",
        "    torch.set_grad_enabled(True)\r\n",
        "    optimizer = torch.optim.SGD(\r\n",
        "        filter(lambda x: x.requires_grad, model.parameters()),\r\n",
        "        lr=0.05,\r\n",
        "        momentum=0.9,\r\n",
        "        weight_decay=10**(-5)\r\n",
        "    )\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    criterion = criterion.to(device)\r\n",
        "\r\n",
        "    for e in range(epoch):\r\n",
        "\r\n",
        "      epoch_loss = 0.0\r\n",
        "\r\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "          data, target = data.to(device), target.to(device)\r\n",
        "          optimizer.zero_grad()\r\n",
        "          output = model(data)\r\n",
        "          loss = criterion(output, target)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "          epoch_loss += output.shape[0] * loss.item()\r\n",
        "\r\n",
        "      print(\"Epoch Nr: \" + str(e))\r\n",
        "      print(epoch_loss / len(train_loader.dataset))\r\n",
        "            \r\n",
        "\r\n",
        "def test(model, device, test_loader):\r\n",
        "    model.eval()\r\n",
        "    test_loss = 0\r\n",
        "    correct = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in test_loader:\r\n",
        "            data, target = data.to(device), target.to(device)\r\n",
        "            \r\n",
        "            output = model(data)\r\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\r\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\r\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, correct, len(test_loader.dataset),\r\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atEkxlBJXGty"
      },
      "source": [
        "# choose device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvWR_NMIfrqL"
      },
      "source": [
        "For simplicity, the whole analysis is done on the mnist dataset.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfF-9ZXjXSyR"
      },
      "source": [
        "transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "        ])\r\n",
        "mnist_train = datasets.MNIST('../data', train=True, download=True,\r\n",
        "                       transform=transform)\r\n",
        "mnist_test = datasets.MNIST('../data', train=False,\r\n",
        "                       transform=transform)\r\n",
        "\r\n",
        "# data is splitted in 3 datasets\r\n",
        "# 1) 55k images - used for unsupervised training \r\n",
        "# 2) 5k images - used for training of linear calssifier on top of features extracted from network trained with DeepCluster\r\n",
        "# 3) 10k images - test set\r\n",
        "\r\n",
        "unsupervised_pretrain, supervised_train = torch.utils.data.random_split(mnist_train, [55000, 5000])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_loader_unsupervised = torch.utils.data.DataLoader(unsupervised_pretrain, batch_size=64,\r\n",
        "                                             shuffle=False, num_workers=4)\r\n",
        "\r\n",
        "train_loader_supervised = torch.utils.data.DataLoader(supervised_train, batch_size=64,\r\n",
        "                                             shuffle=False, num_workers=4)\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64,\r\n",
        "                                             shuffle=True, num_workers=4)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Fm2XH3BePi"
      },
      "source": [
        "The following code snippets are taken from the Deepcluster github repository (https://github.com/facebookresearch/deepcluster) an adapted to the task stated above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLC6IGMjwvd"
      },
      "source": [
        "def cluster_assign(images_lists, dataset):\r\n",
        "    \"\"\"Creates a dataset from clustering, with clusters as labels.\r\n",
        "    Args:\r\n",
        "        images_lists (list of list): for each cluster, the list of image indexes\r\n",
        "                                    belonging to this cluster\r\n",
        "        dataset (list): initial dataset\r\n",
        "    Returns:\r\n",
        "        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\r\n",
        "                                                     labels\r\n",
        "    \"\"\"\r\n",
        "    assert images_lists is not None\r\n",
        "    pseudolabels = []\r\n",
        "    image_indexes = []\r\n",
        "    for cluster, images in enumerate(images_lists):\r\n",
        "        image_indexes.extend(images)\r\n",
        "        pseudolabels.extend([cluster] * len(images))\r\n",
        "\r\n",
        "    t = transforms.Compose([\r\n",
        "               transforms.ToTensor(),\r\n",
        "               transforms.Normalize(mean=(0.1307,), std=(0.3081,))]\r\n",
        "           )\r\n",
        "\r\n",
        "    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NT6Kb8lmVW_"
      },
      "source": [
        "class ReassignedDataset(data.Dataset):\r\n",
        "    \"\"\"A dataset where the new images labels are given in argument. This assigns\r\n",
        "    each image withits \"pseudolabel\"\r\n",
        "    Args:\r\n",
        "        image_indexes (list): list of data indexes\r\n",
        "        pseudolabels (list): list of labels for each data\r\n",
        "        dataset (list): list of tuples with paths to images\r\n",
        "        transform (callable, optional): a function/transform that takes in\r\n",
        "                                        an PIL image and returns a\r\n",
        "                                        transformed version\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, image_indexes, pseudolabels, dataset, transform=None):\r\n",
        "        self.imgs = self.make_dataset(image_indexes, pseudolabels, dataset)\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def make_dataset(self, image_indexes, pseudolabels, dataset):\r\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(set(pseudolabels))}\r\n",
        "        images = []\r\n",
        "        for j, idx in enumerate(image_indexes):\r\n",
        "            path = dataset[idx][0]\r\n",
        "            pseudolabel = label_to_idx[pseudolabels[j]]\r\n",
        "            images.append((path, pseudolabel))\r\n",
        "        return images\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            index (int): index of data\r\n",
        "        Returns:\r\n",
        "            tuple: (image, pseudolabel) where pseudolabel is the cluster of index datapoint\r\n",
        "        \"\"\"\r\n",
        "        img, pseudolabel = self.imgs[index]\r\n",
        "        return img, pseudolabel\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEPuZZ-rn40U"
      },
      "source": [
        "class UnifLabelSampler(Sampler):\r\n",
        "    \"\"\"Samples elements uniformely accross pseudolabels.\r\n",
        "        Args:\r\n",
        "            N (int): size of returned iterator.\r\n",
        "            images_lists: dict of key (target), value (list of data with this target)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, N, images_lists):\r\n",
        "        self.N = N\r\n",
        "        self.images_lists = images_lists\r\n",
        "        self.indexes = self.generate_indexes_epoch()\r\n",
        "\r\n",
        "    def generate_indexes_epoch(self):\r\n",
        "        nmb_non_empty_clusters = 0\r\n",
        "        for i in range(len(self.images_lists)):\r\n",
        "            if len(self.images_lists[i]) != 0:\r\n",
        "                nmb_non_empty_clusters += 1\r\n",
        "\r\n",
        "        size_per_pseudolabel = int(self.N / nmb_non_empty_clusters) + 1\r\n",
        "        res = np.array([])\r\n",
        "\r\n",
        "        for i in range(len(self.images_lists)):\r\n",
        "            # skip empty clusters\r\n",
        "            if len(self.images_lists[i]) == 0:\r\n",
        "                continue\r\n",
        "            indexes = np.random.choice(\r\n",
        "                self.images_lists[i],\r\n",
        "                size_per_pseudolabel,\r\n",
        "                replace=(len(self.images_lists[i]) <= size_per_pseudolabel)\r\n",
        "            )\r\n",
        "            res = np.concatenate((res, indexes))\r\n",
        "\r\n",
        "        np.random.shuffle(res)\r\n",
        "        res = list(res.astype('int'))\r\n",
        "        if len(res) >= self.N:\r\n",
        "            return res[:self.N]\r\n",
        "        res += res[: (self.N - len(res))]\r\n",
        "        return res\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        return iter(self.indexes)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.indexes)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsERBDK2IYlb"
      },
      "source": [
        "class AverageMeter(object):\r\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def update(self, val, n=1):\r\n",
        "        self.val = val\r\n",
        "        self.sum += val * n\r\n",
        "        self.count += n\r\n",
        "        self.avg = self.sum / self.count\r\n",
        "\r\n",
        "\r\n",
        "def learning_rate_decay(optimizer, t, lr_0):\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        lr = lr_0 / np.sqrt(1 + lr_0 * param_group['weight_decay'] * t)\r\n",
        "        param_group['lr'] = lr\r\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6YXCLvcoJQi"
      },
      "source": [
        "def compute_features(dataloader, model, N, get_labels=False):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    labels = []\r\n",
        "\r\n",
        "    # discard the label information in the dataloader\r\n",
        "    for i, (input_tensor, label) in enumerate(dataloader):\r\n",
        "        input_var = torch.autograd.Variable(input_tensor.cuda(), requires_grad=False)\r\n",
        "        aux = model(input_var).data.cpu().numpy()\r\n",
        "\r\n",
        "        if i == 0:\r\n",
        "            features = np.zeros((N, aux.shape[1]), dtype='float32')\r\n",
        "\r\n",
        "        aux = aux.astype('float32')\r\n",
        "        if i < len(dataloader) - 1:\r\n",
        "            features[i * 64: (i + 1) * 64] = aux\r\n",
        "        else:\r\n",
        "            # special treatment for final batch\r\n",
        "            features[i * 64:] = aux\r\n",
        "\r\n",
        "        # measure elapsed time\r\n",
        "\r\n",
        "        labels.append(label.numpy())\r\n",
        "\r\n",
        "    labels = np.concatenate(labels)\r\n",
        "\r\n",
        "    if get_labels:\r\n",
        "      return features, labels\r\n",
        "    \r\n",
        "    else:\r\n",
        "      return features\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrMDcS4pVJS"
      },
      "source": [
        "def train(loader, model, crit, opt, epoch):\r\n",
        "    \"\"\"Training of the CNN.\r\n",
        "        Args:\r\n",
        "            loader (torch.utils.data.DataLoader): Data loader\r\n",
        "            model (nn.Module): CNN\r\n",
        "            crit (torch.nn): loss\r\n",
        "            opt (torch.optim.SGD): optimizer for every parameters with True\r\n",
        "                                   requires_grad in model except top layer\r\n",
        "            epoch (int)\r\n",
        "    \"\"\"\r\n",
        "    losses = AverageMeter()\r\n",
        "    # switch to train mode\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # create an optimizer for the last fc layer\r\n",
        "    optimizer_tl = torch.optim.SGD(\r\n",
        "        model.top_layer.parameters(),\r\n",
        "        lr=0.01,\r\n",
        "        weight_decay=10**-5,\r\n",
        "    )\r\n",
        "\r\n",
        "    for i, (input_tensor, target) in enumerate(loader):\r\n",
        "\r\n",
        "        target = target.cuda(async=True)\r\n",
        "        input_var = torch.autograd.Variable(input_tensor.cuda())\r\n",
        "        target_var = torch.autograd.Variable(target)\r\n",
        "\r\n",
        "        output = model(input_var)\r\n",
        "        loss = crit(output, target_var)\r\n",
        "\r\n",
        "        # record loss\r\n",
        "        losses.update(loss.data, input_tensor.size(0))\r\n",
        "\r\n",
        "        # compute gradient and do SGD step\r\n",
        "        opt.zero_grad()\r\n",
        "        optimizer_tl.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        opt.step()\r\n",
        "        optimizer_tl.step()\r\n",
        "\r\n",
        "    return losses.avg"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIyA5jLYeSz"
      },
      "source": [
        "def DeepCluster(model, device, train_loader, epoch, k):\r\n",
        "\r\n",
        "    fd = int(model.top_layer.weight.size()[1])\r\n",
        "    model.top_layer = None\r\n",
        "\r\n",
        "    model = model.to(device)\r\n",
        "\r\n",
        "\r\n",
        "    optimizer = torch.optim.SGD(\r\n",
        "        filter(lambda x: x.requires_grad, model.parameters()),\r\n",
        "        lr=0.05,\r\n",
        "        momentum=0.9,\r\n",
        "        weight_decay=10**(-5)\r\n",
        "    )\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    criterion = criterion.to(device)\r\n",
        "    #cluster_step\r\n",
        "\r\n",
        "\r\n",
        "    for e in range(epoch):\r\n",
        "         \r\n",
        "      model.top_layer = None\r\n",
        "      model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\r\n",
        "\r\n",
        "      features = compute_features(train_loader, model, len(unsupervised_pretrain))\r\n",
        "\r\n",
        "       # only 64 dims, so no PCA\r\n",
        "      pipeline = Pipeline([('scaling', StandardScaler())])\r\n",
        "      \r\n",
        "      post_scale = pipeline.fit_transform(features)\r\n",
        "      post_norm = normalize(post_scale, norm=\"l2\")\r\n",
        "\r\n",
        "      n_data, d = post_norm.shape\r\n",
        "\r\n",
        "      # faiss implementation of k-means\r\n",
        "      clus = faiss.Clustering(d, k)\r\n",
        "      clus.seed = np.random.randint(1234)\r\n",
        "\r\n",
        "      clus.niter = 20\r\n",
        "      clus.max_points_per_centroid = 60000\r\n",
        "\r\n",
        "      res = faiss.StandardGpuResources()\r\n",
        "      flat_config = faiss.GpuIndexFlatConfig()\r\n",
        "      flat_config.useFloat16 = False\r\n",
        "      flat_config.device = 0\r\n",
        "      index = faiss.GpuIndexFlatL2(res, d, flat_config)\r\n",
        "\r\n",
        "      #get new cluster labels\r\n",
        "      clus.train(post_norm, index)\r\n",
        "      _, I = index.search(post_norm, 1)\r\n",
        "\r\n",
        "      labels = np.squeeze(I)\r\n",
        "\r\n",
        "      unique, counts = np.unique(labels, return_counts=True)\r\n",
        "\r\n",
        "      print(\"Epoch Nr: \" + str(e))\r\n",
        "\r\n",
        "      print(\"Overview of cluster assignments:\")\r\n",
        "      print(dict(zip(unique, counts)))\r\n",
        "\r\n",
        "      images_lists = [[] for i in range(k)]\r\n",
        "      for i in range(len(unsupervised_pretrain)):\r\n",
        "            images_lists[int(labels[i])].append(i)\r\n",
        "\r\n",
        "\r\n",
        "      # create new dataset from pseudolabels\r\n",
        "      train_dataset = cluster_assign(images_lists, unsupervised_pretrain)\r\n",
        "\r\n",
        "      #print(len(train_dataset))\r\n",
        "      #print(images_lists)\r\n",
        "\r\n",
        "      # sample images from uniform distribution over classes\r\n",
        "      sampler = UnifLabelSampler(int(1 * len(train_dataset)),\r\n",
        "                                   images_lists)\r\n",
        "\r\n",
        "\r\n",
        "      train_dataloader = torch.utils.data.DataLoader(\r\n",
        "            train_dataset,\r\n",
        "            batch_size=64,\r\n",
        "            num_workers=4,\r\n",
        "            sampler=sampler,\r\n",
        "        )\r\n",
        "      \r\n",
        "      # reset last layer\r\n",
        "      mlp = list(model.classifier.children())\r\n",
        "      mlp.append(nn.ReLU(inplace=True).cuda())\r\n",
        "      model.classifier = nn.Sequential(*mlp)\r\n",
        "      model.top_layer = nn.Linear(fd, k)\r\n",
        "      model.top_layer.weight.data.normal_(0, 0.01)\r\n",
        "      model.top_layer.bias.data.zero_()\r\n",
        "      model.top_layer.cuda()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      # train step\r\n",
        "      torch.set_grad_enabled(True)\r\n",
        "      loss = train(train_dataloader, model, criterion, optimizer, e)\r\n",
        "      print(loss.cpu().numpy())\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krRxtQXLmOd1"
      },
      "source": [
        "def linear_model(model_base, train_loader, test_loader):\r\n",
        "\r\n",
        "  model = copy.deepcopy(model_base)\r\n",
        "  model.to(device)\r\n",
        "  model.top_layer = None\r\n",
        "  model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\r\n",
        "  features,labels = compute_features(train_loader, model, len(supervised_train), get_labels=True)\r\n",
        "\r\n",
        "  clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5, max_iter =10000))\r\n",
        "  clf.fit(features, labels)\r\n",
        "\r\n",
        "  x_test = []\r\n",
        "  y_true = []\r\n",
        "\r\n",
        "  torch.set_grad_enabled(False)\r\n",
        "  for idx, (pics, labels) in enumerate(test_loader):\r\n",
        "    pics = pics.to(device)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    features_test = model(pics)\r\n",
        "    x_test.append(features_test.cpu().numpy())\r\n",
        "    y_true.append(labels)\r\n",
        "\r\n",
        "  x_test = np.concatenate(x_test)\r\n",
        "  y_true = np.concatenate(y_true)\r\n",
        "\r\n",
        "  y_pred = clf.predict(x_test)\r\n",
        "\r\n",
        "  print(\"Test Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfzxPTFEmYhq"
      },
      "source": [
        "Train CNN in self supervised manner using deepcluster (55k images) - 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWHc9bqzZ-iM",
        "outputId": "f7ddbf12-7cc1-40d6-86d7-e1c6d39b1083"
      },
      "source": [
        "simpleCNN = SimpleCnn()\r\n",
        "simpleCNN = simpleCNN.to(device)\r\n",
        "DeepCluster(simpleCNN, device, train_loader_unsupervised, 5, 10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Nr: 0\n",
            "Overview of cluster assignments:\n",
            "{0: 5190, 1: 8158, 2: 5186, 3: 4494, 4: 4624, 5: 6721, 6: 4508, 7: 4950, 8: 6502, 9: 4667}\n",
            "0.38349342\n",
            "Epoch Nr: 1\n",
            "Overview of cluster assignments:\n",
            "{0: 5917, 1: 3883, 2: 4430, 3: 4986, 4: 7153, 5: 6829, 6: 4684, 7: 7281, 8: 4374, 9: 5463}\n",
            "0.17632245\n",
            "Epoch Nr: 2\n",
            "Overview of cluster assignments:\n",
            "{0: 6319, 1: 5018, 2: 5938, 3: 5430, 4: 5202, 5: 5035, 6: 5730, 7: 4609, 8: 4947, 9: 6772}\n",
            "0.14299005\n",
            "Epoch Nr: 3\n",
            "Overview of cluster assignments:\n",
            "{0: 6513, 1: 5148, 2: 5927, 3: 5439, 4: 5710, 5: 6013, 6: 5373, 7: 5326, 8: 5103, 9: 4448}\n",
            "0.13208367\n",
            "Epoch Nr: 4\n",
            "Overview of cluster assignments:\n",
            "{0: 6169, 1: 6297, 2: 4642, 3: 5065, 4: 4685, 5: 5275, 6: 6026, 7: 5945, 8: 5456, 9: 5440}\n",
            "0.14878279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfyuOMxBnk94"
      },
      "source": [
        "Initialize a model with random weights (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX6K1QFPmhzy"
      },
      "source": [
        "random_CNN = SimpleCnn()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrCWVkd5n0Pl"
      },
      "source": [
        "Initialize a model that is trained in a supervised manner on the 55k train images (should act as an upper bound for the performance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzyhHA-unyOf",
        "outputId": "e6f2a21d-5d14-4cda-89a7-40a79ddb9e5b"
      },
      "source": [
        "trainCNN = SimpleCnn()\r\n",
        "trainCNN = trainCNN.to(device)\r\n",
        "train_supervised(trainCNN, device, train_loader_unsupervised, 5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Nr: 0\n",
            "0.1711980837157504\n",
            "Epoch Nr: 1\n",
            "0.052250907634063204\n",
            "Epoch Nr: 2\n",
            "0.036913110028275034\n",
            "Epoch Nr: 3\n",
            "0.029363632428578354\n",
            "Epoch Nr: 4\n",
            "0.024624731658178974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUkYu3qwKYG"
      },
      "source": [
        "Compare the results of a linear model trained on the features of the fist fc layer (5000 labeled images). Evaluated on test set (10000 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZb6EQr189G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03bf1d2-8b41-486c-971c-b988f66e2120"
      },
      "source": [
        "# random weight cnn (lower bound)\r\n",
        "linear_model(random_CNN, train_loader_supervised, test_loader)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ygVBsH1gek",
        "outputId": "8ece0fbf-b04e-4ff1-b503-91ec4ba26ac4"
      },
      "source": [
        "# cnn trained self supervised\r\n",
        "linear_model(simpleCNN, train_loader_supervised, test_loader)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K6DGuqJNbjp",
        "outputId": "1a36b979-745c-4d57-934c-1de47d4c7a86"
      },
      "source": [
        "# cnn trained supervised\r\n",
        "linear_model(trainCNN, train_loader_supervised, test_loader)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLlQVZk31_qY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}