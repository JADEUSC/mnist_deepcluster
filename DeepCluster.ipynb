{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepCluster.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mq8IuIxVW-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7a922a-3b9d-424d-c7a9-f79a9d996f77"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import datasets, transforms\r\n",
        "from torch.optim.lr_scheduler import StepLR\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import StandardScaler, normalize\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.cluster import KMeans\r\n",
        "from sklearn.linear_model import LinearRegression\r\n",
        "import torch.utils.data as data\r\n",
        "from torch.utils.data.sampler import Sampler\r\n",
        "import math\r\n",
        "import copy\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "!pip install faiss-gpu\r\n",
        "import faiss\r\n",
        "\r\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.6/dist-packages (1.6.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f23b4a07d98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BllDgfIse7jz"
      },
      "source": [
        "The model used in this analysis is a simple cnn model with 2 convolutional layer and two fully connected layers. The model is split into features, classifier and top_layer to mimic the architecture used in the original paper. See [here](https://github.com/facebookresearch/deepcluster/tree/master/models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQTYT7tAWk8T"
      },
      "source": [
        "class SimpleCnn(nn.Module):\r\n",
        "    \r\n",
        "    def __init__(self, k=10):\r\n",
        "        \r\n",
        "        super(SimpleCnn, self).__init__()\r\n",
        "        self.features = nn.Sequential(\r\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(8),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\r\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(16),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\r\n",
        "        \r\n",
        "        self.classifier = nn.Sequential(\r\n",
        "            nn.Linear(7*7*16, 64),\r\n",
        "            nn.ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.top_layer = nn.Linear(64, k)\r\n",
        "        self._initialize_weights()\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        out = self.features(x)\r\n",
        "        out = out.reshape(out.size(0), -1)\r\n",
        "        out = self.classifier(out)\r\n",
        "        if self.top_layer:\r\n",
        "            out = self.top_layer(out)\r\n",
        "        return out\r\n",
        "    \r\n",
        "    def _initialize_weights(self):\r\n",
        "        for y, m in enumerate(self.modules()):\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                for i in range(m.out_channels):\r\n",
        "                    m.weight.data[i].normal_(0, math.sqrt(2. / n))\r\n",
        "                if m.bias is not None:\r\n",
        "                    m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                m.weight.data.normal_(0, 0.01)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "def train_supervised(model, device, train_loader, epoch):\r\n",
        "    model.train()\r\n",
        "    torch.set_grad_enabled(True)\r\n",
        "    optimizer = torch.optim.SGD(\r\n",
        "        filter(lambda x: x.requires_grad, model.parameters()),\r\n",
        "        lr=0.05,\r\n",
        "        momentum=0.9,\r\n",
        "        weight_decay=10**(-5)\r\n",
        "    )\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    criterion = criterion.to(device)\r\n",
        "\r\n",
        "    for e in range(epoch):\r\n",
        "\r\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "          data, target = data.to(device), target.to(device)\r\n",
        "          optimizer.zero_grad()\r\n",
        "          output = model(data)\r\n",
        "          loss = criterion(output, target)\r\n",
        "          loss.backward()\r\n",
        "          optimizer.step()\r\n",
        "          if batch_idx % 100 == 0:\r\n",
        "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n",
        "                  e, batch_idx * len(data), len(train_loader.dataset),\r\n",
        "                  100. * batch_idx / len(train_loader), loss.item()))\r\n",
        "            \r\n",
        "\r\n",
        "def test(model, device, test_loader):\r\n",
        "    model.eval()\r\n",
        "    test_loss = 0\r\n",
        "    correct = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for data, target in test_loader:\r\n",
        "            data, target = data.to(device), target.to(device)\r\n",
        "            \r\n",
        "            output = model(data)\r\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\r\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\r\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\r\n",
        "\r\n",
        "    test_loss /= len(test_loader.dataset)\r\n",
        "\r\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n",
        "        test_loss, correct, len(test_loader.dataset),\r\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atEkxlBJXGty"
      },
      "source": [
        "# choose device\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvWR_NMIfrqL"
      },
      "source": [
        "For simplicity, the whole analysis is done on the mnist dataset.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfF-9ZXjXSyR"
      },
      "source": [
        "transform=transforms.Compose([\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\r\n",
        "        ])\r\n",
        "mnist_train = datasets.MNIST('../data', train=True, download=True,\r\n",
        "                       transform=transform)\r\n",
        "mnist_test = datasets.MNIST('../data', train=False,\r\n",
        "                       transform=transform)\r\n",
        "\r\n",
        "# data is splitted in 3 datasets\r\n",
        "# 1) 55k images - used for unsupervised training \r\n",
        "# 2) 5k images - used for training of linear calssifier on top of features extracted from network trained with DeepCluster\r\n",
        "# 3) 10k images - test set\r\n",
        "\r\n",
        "unsupervised_pretrain, supervised_train = torch.utils.data.random_split(mnist_train, [55000, 5000])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "train_loader_unsupervised = torch.utils.data.DataLoader(unsupervised_pretrain, batch_size=64,\r\n",
        "                                             shuffle=False, num_workers=4)\r\n",
        "\r\n",
        "train_loader_supervised = torch.utils.data.DataLoader(supervised_train, batch_size=64,\r\n",
        "                                             shuffle=False, num_workers=4)\r\n",
        "\r\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64,\r\n",
        "                                             shuffle=True, num_workers=4)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Fm2XH3BePi"
      },
      "source": [
        "The following code snippets are taken from the Deepcluster github repository (https://github.com/facebookresearch/deepcluster) an adapted to the task stated above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfLC6IGMjwvd"
      },
      "source": [
        "def cluster_assign(images_lists, dataset):\r\n",
        "    \"\"\"Creates a dataset from clustering, with clusters as labels.\r\n",
        "    Args:\r\n",
        "        images_lists (list of list): for each cluster, the list of image indexes\r\n",
        "                                    belonging to this cluster\r\n",
        "        dataset (list): initial dataset\r\n",
        "    Returns:\r\n",
        "        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\r\n",
        "                                                     labels\r\n",
        "    \"\"\"\r\n",
        "    assert images_lists is not None\r\n",
        "    pseudolabels = []\r\n",
        "    image_indexes = []\r\n",
        "    for cluster, images in enumerate(images_lists):\r\n",
        "        image_indexes.extend(images)\r\n",
        "        pseudolabels.extend([cluster] * len(images))\r\n",
        "\r\n",
        "    t = transforms.Compose([\r\n",
        "               transforms.ToTensor(),\r\n",
        "               transforms.Normalize(mean=(0.1307,), std=(0.3081,))]\r\n",
        "           )\r\n",
        "\r\n",
        "    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)\r\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NT6Kb8lmVW_"
      },
      "source": [
        "class ReassignedDataset(data.Dataset):\r\n",
        "    \"\"\"A dataset where the new images labels are given in argument. This assigns\r\n",
        "    each image withits \"pseudolabel\"\r\n",
        "    Args:\r\n",
        "        image_indexes (list): list of data indexes\r\n",
        "        pseudolabels (list): list of labels for each data\r\n",
        "        dataset (list): list of tuples with paths to images\r\n",
        "        transform (callable, optional): a function/transform that takes in\r\n",
        "                                        an PIL image and returns a\r\n",
        "                                        transformed version\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, image_indexes, pseudolabels, dataset, transform=None):\r\n",
        "        self.imgs = self.make_dataset(image_indexes, pseudolabels, dataset)\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "    def make_dataset(self, image_indexes, pseudolabels, dataset):\r\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(set(pseudolabels))}\r\n",
        "        images = []\r\n",
        "        for j, idx in enumerate(image_indexes):\r\n",
        "            path = dataset[idx][0]\r\n",
        "            pseudolabel = label_to_idx[pseudolabels[j]]\r\n",
        "            images.append((path, pseudolabel))\r\n",
        "        return images\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            index (int): index of data\r\n",
        "        Returns:\r\n",
        "            tuple: (image, pseudolabel) where pseudolabel is the cluster of index datapoint\r\n",
        "        \"\"\"\r\n",
        "        img, pseudolabel = self.imgs[index]\r\n",
        "        return img, pseudolabel\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEPuZZ-rn40U"
      },
      "source": [
        "class UnifLabelSampler(Sampler):\r\n",
        "    \"\"\"Samples elements uniformely accross pseudolabels.\r\n",
        "        Args:\r\n",
        "            N (int): size of returned iterator.\r\n",
        "            images_lists: dict of key (target), value (list of data with this target)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, N, images_lists):\r\n",
        "        self.N = N\r\n",
        "        self.images_lists = images_lists\r\n",
        "        self.indexes = self.generate_indexes_epoch()\r\n",
        "\r\n",
        "    def generate_indexes_epoch(self):\r\n",
        "        nmb_non_empty_clusters = 0\r\n",
        "        for i in range(len(self.images_lists)):\r\n",
        "            if len(self.images_lists[i]) != 0:\r\n",
        "                nmb_non_empty_clusters += 1\r\n",
        "\r\n",
        "        size_per_pseudolabel = int(self.N / nmb_non_empty_clusters) + 1\r\n",
        "        res = np.array([])\r\n",
        "\r\n",
        "        for i in range(len(self.images_lists)):\r\n",
        "            # skip empty clusters\r\n",
        "            if len(self.images_lists[i]) == 0:\r\n",
        "                continue\r\n",
        "            indexes = np.random.choice(\r\n",
        "                self.images_lists[i],\r\n",
        "                size_per_pseudolabel,\r\n",
        "                replace=(len(self.images_lists[i]) <= size_per_pseudolabel)\r\n",
        "            )\r\n",
        "            res = np.concatenate((res, indexes))\r\n",
        "\r\n",
        "        np.random.shuffle(res)\r\n",
        "        res = list(res.astype('int'))\r\n",
        "        if len(res) >= self.N:\r\n",
        "            return res[:self.N]\r\n",
        "        res += res[: (self.N - len(res))]\r\n",
        "        return res\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        return iter(self.indexes)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.indexes)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsERBDK2IYlb"
      },
      "source": [
        "class AverageMeter(object):\r\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.reset()\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        self.val = 0\r\n",
        "        self.avg = 0\r\n",
        "        self.sum = 0\r\n",
        "        self.count = 0\r\n",
        "\r\n",
        "    def update(self, val, n=1):\r\n",
        "        self.val = val\r\n",
        "        self.sum += val * n\r\n",
        "        self.count += n\r\n",
        "        self.avg = self.sum / self.count\r\n",
        "\r\n",
        "\r\n",
        "def learning_rate_decay(optimizer, t, lr_0):\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        lr = lr_0 / np.sqrt(1 + lr_0 * param_group['weight_decay'] * t)\r\n",
        "        param_group['lr'] = lr\r\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6YXCLvcoJQi"
      },
      "source": [
        "def compute_features(dataloader, model, N, get_labels=False):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    labels = []\r\n",
        "\r\n",
        "    # discard the label information in the dataloader\r\n",
        "    for i, (input_tensor, label) in enumerate(dataloader):\r\n",
        "        input_var = torch.autograd.Variable(input_tensor.cuda(), requires_grad=False)\r\n",
        "        aux = model(input_var).data.cpu().numpy()\r\n",
        "\r\n",
        "        if i == 0:\r\n",
        "            features = np.zeros((N, aux.shape[1]), dtype='float32')\r\n",
        "\r\n",
        "        aux = aux.astype('float32')\r\n",
        "        if i < len(dataloader) - 1:\r\n",
        "            features[i * 64: (i + 1) * 64] = aux\r\n",
        "        else:\r\n",
        "            # special treatment for final batch\r\n",
        "            features[i * 64:] = aux\r\n",
        "\r\n",
        "        # measure elapsed time\r\n",
        "\r\n",
        "        labels.append(label.numpy())\r\n",
        "\r\n",
        "    labels = np.concatenate(labels)\r\n",
        "\r\n",
        "    if get_labels:\r\n",
        "      return features, labels\r\n",
        "    \r\n",
        "    else:\r\n",
        "      return features\r\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBrMDcS4pVJS"
      },
      "source": [
        "def train(loader, model, crit, opt, epoch):\r\n",
        "    \"\"\"Training of the CNN.\r\n",
        "        Args:\r\n",
        "            loader (torch.utils.data.DataLoader): Data loader\r\n",
        "            model (nn.Module): CNN\r\n",
        "            crit (torch.nn): loss\r\n",
        "            opt (torch.optim.SGD): optimizer for every parameters with True\r\n",
        "                                   requires_grad in model except top layer\r\n",
        "            epoch (int)\r\n",
        "    \"\"\"\r\n",
        "    losses = AverageMeter()\r\n",
        "    # switch to train mode\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # create an optimizer for the last fc layer\r\n",
        "    optimizer_tl = torch.optim.SGD(\r\n",
        "        model.top_layer.parameters(),\r\n",
        "        lr=0.01,\r\n",
        "        weight_decay=10**-5,\r\n",
        "    )\r\n",
        "\r\n",
        "    for i, (input_tensor, target) in enumerate(loader):\r\n",
        "\r\n",
        "        target = target.cuda(async=True)\r\n",
        "        input_var = torch.autograd.Variable(input_tensor.cuda())\r\n",
        "        target_var = torch.autograd.Variable(target)\r\n",
        "\r\n",
        "        output = model(input_var)\r\n",
        "        loss = crit(output, target_var)\r\n",
        "\r\n",
        "        # record loss\r\n",
        "        losses.update(loss.data, input_tensor.size(0))\r\n",
        "\r\n",
        "        # compute gradient and do SGD step\r\n",
        "        opt.zero_grad()\r\n",
        "        optimizer_tl.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        opt.step()\r\n",
        "        optimizer_tl.step()\r\n",
        "\r\n",
        "    return losses.avg"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIyA5jLYeSz"
      },
      "source": [
        "def DeepCluster(model, device, train_loader, epoch, k):\r\n",
        "\r\n",
        "    fd = int(model.top_layer.weight.size()[1])\r\n",
        "    model.top_layer = None\r\n",
        "\r\n",
        "    model = model.to(device)\r\n",
        "\r\n",
        "\r\n",
        "    optimizer = torch.optim.SGD(\r\n",
        "        filter(lambda x: x.requires_grad, model.parameters()),\r\n",
        "        lr=0.05,\r\n",
        "        momentum=0.9,\r\n",
        "        weight_decay=10**(-5)\r\n",
        "    )\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    criterion = criterion.to(device)\r\n",
        "    #cluster_step\r\n",
        "\r\n",
        "\r\n",
        "    for e in range(epoch):\r\n",
        "         \r\n",
        "      model.top_layer = None\r\n",
        "      model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\r\n",
        "\r\n",
        "      features = compute_features(train_loader, model, len(unsupervised_pretrain))\r\n",
        "\r\n",
        "       # only 64 dims, so no PCA\r\n",
        "      pipeline = Pipeline([('scaling', StandardScaler())])\r\n",
        "      \r\n",
        "      post_scale = pipeline.fit_transform(features)\r\n",
        "      post_norm = normalize(post_scale, norm=\"l2\")\r\n",
        "\r\n",
        "      n_data, d = post_norm.shape\r\n",
        "\r\n",
        "      # faiss implementation of k-means\r\n",
        "      clus = faiss.Clustering(d, k)\r\n",
        "      clus.seed = np.random.randint(1234)\r\n",
        "\r\n",
        "      clus.niter = 20\r\n",
        "      clus.max_points_per_centroid = 60000\r\n",
        "\r\n",
        "      res = faiss.StandardGpuResources()\r\n",
        "      flat_config = faiss.GpuIndexFlatConfig()\r\n",
        "      flat_config.useFloat16 = False\r\n",
        "      flat_config.device = 0\r\n",
        "      index = faiss.GpuIndexFlatL2(res, d, flat_config)\r\n",
        "\r\n",
        "      #get new cluster labels\r\n",
        "      clus.train(post_norm, index)\r\n",
        "      _, I = index.search(post_norm, 1)\r\n",
        "\r\n",
        "      labels = np.squeeze(I)\r\n",
        "\r\n",
        "      unique, counts = np.unique(labels, return_counts=True)\r\n",
        "      print(dict(zip(unique, counts)))\r\n",
        "\r\n",
        "      images_lists = [[] for i in range(k)]\r\n",
        "      for i in range(len(unsupervised_pretrain)):\r\n",
        "            images_lists[int(labels[i])].append(i)\r\n",
        "\r\n",
        "\r\n",
        "      # create new dataset from pseudolabels\r\n",
        "      train_dataset = cluster_assign(images_lists, unsupervised_pretrain)\r\n",
        "\r\n",
        "      #print(len(train_dataset))\r\n",
        "      #print(images_lists)\r\n",
        "\r\n",
        "      # sample images from uniform distribution over classes\r\n",
        "      sampler = UnifLabelSampler(int(1 * len(train_dataset)),\r\n",
        "                                   images_lists)\r\n",
        "\r\n",
        "\r\n",
        "      train_dataloader = torch.utils.data.DataLoader(\r\n",
        "            train_dataset,\r\n",
        "            batch_size=64,\r\n",
        "            num_workers=4,\r\n",
        "            sampler=sampler,\r\n",
        "        )\r\n",
        "      \r\n",
        "      # reset last layer\r\n",
        "      mlp = list(model.classifier.children())\r\n",
        "      mlp.append(nn.ReLU(inplace=True).cuda())\r\n",
        "      model.classifier = nn.Sequential(*mlp)\r\n",
        "      model.top_layer = nn.Linear(fd, k)\r\n",
        "      model.top_layer.weight.data.normal_(0, 0.01)\r\n",
        "      model.top_layer.bias.data.zero_()\r\n",
        "      model.top_layer.cuda()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      # train step\r\n",
        "      torch.set_grad_enabled(True)\r\n",
        "      loss = train(train_dataloader, model, criterion, optimizer, e)\r\n",
        "      print(\"Epoch Nr:\" + str(e))\r\n",
        "      print(loss.cpu().numpy())\r\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krRxtQXLmOd1"
      },
      "source": [
        "def linear_model(model_base, train_loader, test_loader):\r\n",
        "\r\n",
        "  model = copy.deepcopy(model_base)\r\n",
        "  model.to(device)\r\n",
        "  model.top_layer = None\r\n",
        "  model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\r\n",
        "  features,labels = compute_features(train_loader, model, len(supervised_train), get_labels=True)\r\n",
        "\r\n",
        "  clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5, max_iter =10000))\r\n",
        "  clf.fit(features, labels)\r\n",
        "\r\n",
        "  x_test = []\r\n",
        "  y_true = []\r\n",
        "\r\n",
        "  torch.set_grad_enabled(False)\r\n",
        "  for idx, (pics, labels) in enumerate(test_loader):\r\n",
        "    pics = pics.to(device)\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "    features_test = model(pics)\r\n",
        "    x_test.append(features_test.cpu().numpy())\r\n",
        "    y_true.append(labels)\r\n",
        "\r\n",
        "  x_test = np.concatenate(x_test)\r\n",
        "  y_true = np.concatenate(y_true)\r\n",
        "\r\n",
        "  y_pred = clf.predict(x_test)\r\n",
        "\r\n",
        "  print(\"Test Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfzxPTFEmYhq"
      },
      "source": [
        "Train CNN in self supervised manner using deepcluster (55k images) - 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWHc9bqzZ-iM",
        "outputId": "1272bb9e-ade8-4347-afea-5c83ae85f3d0"
      },
      "source": [
        "simpleCNN = SimpleCnn()\r\n",
        "simpleCNN = simpleCNN.to(device)\r\n",
        "DeepCluster(simpleCNN, device, train_loader_unsupervised, 5, 10)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 4222, 1: 7192, 2: 4011, 3: 6000, 4: 5477, 5: 6803, 6: 4735, 7: 4269, 8: 5453, 9: 6838}\n",
            "Epoch Nr:0\n",
            "0.3598836\n",
            "{0: 7284, 1: 7326, 2: 6206, 3: 5224, 4: 5829, 5: 4062, 6: 4982, 7: 4263, 8: 4475, 9: 5349}\n",
            "Epoch Nr:1\n",
            "0.16842784\n",
            "{0: 4801, 1: 5556, 2: 4300, 3: 4050, 4: 4765, 5: 4738, 6: 5431, 7: 7502, 8: 6148, 9: 7709}\n",
            "Epoch Nr:2\n",
            "0.15201522\n",
            "{0: 4276, 1: 5427, 2: 5259, 3: 6610, 4: 4182, 5: 6070, 6: 7175, 7: 6017, 8: 4843, 9: 5141}\n",
            "Epoch Nr:3\n",
            "0.1853473\n",
            "{0: 5946, 1: 5718, 2: 4478, 3: 5679, 4: 6239, 5: 5843, 6: 6056, 7: 5187, 8: 5059, 9: 4795}\n",
            "Epoch Nr:4\n",
            "0.18122402\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfyuOMxBnk94"
      },
      "source": [
        "Initialize a model with random weights (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX6K1QFPmhzy"
      },
      "source": [
        "random_CNN = SimpleCnn()"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrCWVkd5n0Pl"
      },
      "source": [
        "Initialize a model that is trained in a supervised manner on the 55k train images (should act as an upper bound for the performance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzyhHA-unyOf",
        "outputId": "48bc38ef-6dbb-4ff5-b93d-34bbb2d3c792"
      },
      "source": [
        "trainCNN = SimpleCnn()\r\n",
        "trainCNN = trainCNN.to(device)\r\n",
        "train_supervised(trainCNN, device, train_loader_unsupervised, 5)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/55000 (0%)]\tLoss: 2.304591\n",
            "Train Epoch: 0 [6400/55000 (12%)]\tLoss: 0.373002\n",
            "Train Epoch: 0 [12800/55000 (23%)]\tLoss: 0.138366\n",
            "Train Epoch: 0 [19200/55000 (35%)]\tLoss: 0.077200\n",
            "Train Epoch: 0 [25600/55000 (47%)]\tLoss: 0.062945\n",
            "Train Epoch: 0 [32000/55000 (58%)]\tLoss: 0.045295\n",
            "Train Epoch: 0 [38400/55000 (70%)]\tLoss: 0.010040\n",
            "Train Epoch: 0 [44800/55000 (81%)]\tLoss: 0.053496\n",
            "Train Epoch: 0 [51200/55000 (93%)]\tLoss: 0.094496\n",
            "Train Epoch: 1 [0/55000 (0%)]\tLoss: 0.063225\n",
            "Train Epoch: 1 [6400/55000 (12%)]\tLoss: 0.056392\n",
            "Train Epoch: 1 [12800/55000 (23%)]\tLoss: 0.010412\n",
            "Train Epoch: 1 [19200/55000 (35%)]\tLoss: 0.028235\n",
            "Train Epoch: 1 [25600/55000 (47%)]\tLoss: 0.018711\n",
            "Train Epoch: 1 [32000/55000 (58%)]\tLoss: 0.003925\n",
            "Train Epoch: 1 [38400/55000 (70%)]\tLoss: 0.002388\n",
            "Train Epoch: 1 [44800/55000 (81%)]\tLoss: 0.053427\n",
            "Train Epoch: 1 [51200/55000 (93%)]\tLoss: 0.082242\n",
            "Train Epoch: 2 [0/55000 (0%)]\tLoss: 0.052272\n",
            "Train Epoch: 2 [6400/55000 (12%)]\tLoss: 0.054240\n",
            "Train Epoch: 2 [12800/55000 (23%)]\tLoss: 0.025346\n",
            "Train Epoch: 2 [19200/55000 (35%)]\tLoss: 0.008849\n",
            "Train Epoch: 2 [25600/55000 (47%)]\tLoss: 0.009625\n",
            "Train Epoch: 2 [32000/55000 (58%)]\tLoss: 0.001383\n",
            "Train Epoch: 2 [38400/55000 (70%)]\tLoss: 0.000764\n",
            "Train Epoch: 2 [44800/55000 (81%)]\tLoss: 0.040334\n",
            "Train Epoch: 2 [51200/55000 (93%)]\tLoss: 0.071633\n",
            "Train Epoch: 3 [0/55000 (0%)]\tLoss: 0.079213\n",
            "Train Epoch: 3 [6400/55000 (12%)]\tLoss: 0.029470\n",
            "Train Epoch: 3 [12800/55000 (23%)]\tLoss: 0.016103\n",
            "Train Epoch: 3 [19200/55000 (35%)]\tLoss: 0.009175\n",
            "Train Epoch: 3 [25600/55000 (47%)]\tLoss: 0.015192\n",
            "Train Epoch: 3 [32000/55000 (58%)]\tLoss: 0.002474\n",
            "Train Epoch: 3 [38400/55000 (70%)]\tLoss: 0.003195\n",
            "Train Epoch: 3 [44800/55000 (81%)]\tLoss: 0.030178\n",
            "Train Epoch: 3 [51200/55000 (93%)]\tLoss: 0.032012\n",
            "Train Epoch: 4 [0/55000 (0%)]\tLoss: 0.098081\n",
            "Train Epoch: 4 [6400/55000 (12%)]\tLoss: 0.082061\n",
            "Train Epoch: 4 [12800/55000 (23%)]\tLoss: 0.017914\n",
            "Train Epoch: 4 [19200/55000 (35%)]\tLoss: 0.002034\n",
            "Train Epoch: 4 [25600/55000 (47%)]\tLoss: 0.002819\n",
            "Train Epoch: 4 [32000/55000 (58%)]\tLoss: 0.001640\n",
            "Train Epoch: 4 [38400/55000 (70%)]\tLoss: 0.000490\n",
            "Train Epoch: 4 [44800/55000 (81%)]\tLoss: 0.011024\n",
            "Train Epoch: 4 [51200/55000 (93%)]\tLoss: 0.038220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUkYu3qwKYG"
      },
      "source": [
        "Compare the results of a linear model trained on the features of the fist fc layer (5000 labeled images). Evaluated on test set (10000 images)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZb6EQr189G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2ea28e-5336-4672-f4c1-00144f4c2056"
      },
      "source": [
        "# random weight cnn (lower bound)\r\n",
        "linear_model(random_CNN, train_loader_supervised, test_loader)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ygVBsH1gek",
        "outputId": "b54b0ede-240c-4c98-fcc6-ab21c7ea15b9"
      },
      "source": [
        "# cnn trained self supervised\r\n",
        "linear_model(simpleCNN, train_loader_supervised, test_loader)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K6DGuqJNbjp",
        "outputId": "447da157-6a15-4c58-fac5-85029f5d723b"
      },
      "source": [
        "# cnn trained supervised\r\n",
        "linear_model(trainCNN, train_loader_supervised, test_loader)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLlQVZk31_qY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}